{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "* [Setup](#setup)\n",
    "* Supervised Learning\n",
    "    * [Naive Bayes](#naive-bayes)\n",
    "    * [SVM](#svm)\n",
    "    * [Decision Tree](#decision-tree)\n",
    "    * [KNN](#knn)\n",
    "    * [Random Forest](#random-forest)\n",
    "    * [Adaboost](#adaboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'setup'></a>\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run \"~/github/udacity-dand/machine-learning/ud120-projects/tools/startup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'naive-bayes'></a>\n",
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load nb_author_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    This is the code to accompany the Lesson 1 (Naive Bayes) mini-project.\n",
    "\n",
    "    Use a Naive Bayes Classifier to identify emails by their authors\n",
    "\n",
    "    authors and labels:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = GaussianNB()\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print 'training time: ', round(time() - t0, 3), 's'\n",
    "\n",
    "t1 = time()\n",
    "pred = clf.predict(features_test)\n",
    "print 'predict time:', round(time() - t1, 3), 's'\n",
    "\n",
    "t2 = time()\n",
    "pred_accuracy1 = accuracy_score(pred, labels_test)\n",
    "print 'check time 1: ', round(time() - t2, 3), 's'\n",
    "\n",
    "t3 = time()\n",
    "pred_accuracy2 = clf.score(features_test, labels_test)\n",
    "print 'check time 2: ', round(time() - t3, 3), 's'\n",
    "\n",
    "print(pred_accuracy1)\n",
    "print(pred_accuracy2)\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.metrics.accuracy_score()` is much faster than `clf.scores()`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'svm'></a>\n",
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load svm_author_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print 'the fitting takes: ', round(time() - t0), 's'\n",
    "\n",
    "t1 = time()\n",
    "pre = clf.predict(features_test)\n",
    "print 'the predict: ', round(time() - t1), 's'\n",
    "\n",
    "acc = accuracy_score(pre, labels_test)\n",
    "print acc\n",
    "\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very good rate, but also very slow. Let's try some another way to speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load svm_author_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# use 1% of data\n",
    "\n",
    "features_train = features_train[:len(features_train)/100]\n",
    "labels_train = labels_train[:len(labels_train)/100]\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print 'the fitting takes: ', round(time() - t0), 's'\n",
    "\n",
    "t1 = time()\n",
    "pre = clf.predict(features_test)\n",
    "print 'the predict: ', round(time() - t1), 's'\n",
    "\n",
    "acc = accuracy_score(pre, labels_test)\n",
    "print acc\n",
    "\n",
    "\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a tradeoff between speed and accuracy. Let's try another kind of SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load svm_author_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "features_train = features_train[:len(features_train)/100]\n",
    "labels_train = labels_train[:len(labels_train)/100]\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print 'the fitting takes: ', round(time() - t0), 's'\n",
    "\n",
    "t1 = time()\n",
    "pre = clf.predict(features_test)\n",
    "print 'the predict: ', round(time() - t1), 's'\n",
    "\n",
    "acc = accuracy_score(pre, labels_test)\n",
    "print acc\n",
    "\n",
    "\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a very good accuracy rate. Let's play around with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load svm_author_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(kernel='rbf', C = 10000)\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print 'the fitting takes: ', round(time() - t0), 's'\n",
    "\n",
    "t1 = time()\n",
    "pre = clf.predict(features_test)\n",
    "print 'the predict: ', round(time() - t1), 's'\n",
    "\n",
    "acc = accuracy_score(pre, labels_test)\n",
    "print acc\n",
    "\n",
    "\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under 1% testing data:\n",
    "\n",
    "|C|Accuracy|Time|\n",
    "|---|---|---|\n",
    "|10|0.616040955631|0s|\n",
    "|100|0.616040955631|0s|\n",
    "|1000|0.821387940842|0s|\n",
    "|100000|0.892491467577|0s|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(features_test[[10, 26, 50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pre == 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'decision-tree'></a>\n",
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load dt_author_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    This is the code to accompany the Lesson 3 (decision tree) mini-project.\n",
    "\n",
    "    Use a Decision Tree to identify emails from the Enron corpus by author:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split=40)\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"the fitting takes: \", round(time() - t0), 's'\n",
    "\n",
    "t1 = time()\n",
    "pre = clf.predict(features_test)\n",
    "print \"the test takes: \", round(time() - t1), 's'\n",
    "\n",
    "accuracy_score(pre, labels_test)\n",
    "\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the percentile in email.preprocess.py from 10 to 1 to adjust the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load dt_author_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    This is the code to accompany the Lesson 3 (decision tree) mini-project.\n",
    "\n",
    "    Use a Decision Tree to identify emails from the Enron corpus by author:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split=40)\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"the fitting takes: \", round(time() - t0), 's'\n",
    "\n",
    "t1 = time()\n",
    "pre = clf.predict(features_test)\n",
    "print \"the test takes: \", round(time() - t1), 's'\n",
    "\n",
    "accuracy_score(pre, labels_test)\n",
    "\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email_preprocess import preprocess\n",
    "features_train, features_test, labels_train, labels_test = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /Users/guanrongfu/ml-practice/tools/email_preprocess.py\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split=40)\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"the fitting takes: \", round(time() - t0), 's'\n",
    "\n",
    "t1 = time()\n",
    "pre = clf.predict(features_test)\n",
    "print \"the test takes: \", round(time() - t1), 's'\n",
    "\n",
    "accuracy_score(pre, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'knn'></a>\n",
    "# K Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load knn_author_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "neigh = KNeighborsClassifier()\n",
    "\n",
    "t0 = time()\n",
    "neigh.fit(features_train, labels_train)\n",
    "print 'the fitting takes: ', round(time() - t0), 's'\n",
    "\n",
    "t1 = time()\n",
    "pre = neigh.predict(features_test)\n",
    "print 'the predict: ', round(time() - t1), 's'\n",
    "\n",
    "acc = accuracy_score(pre, labels_test)\n",
    "print acc\n",
    "\n",
    "\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN has a function that the previous three models doesn't. It is the predict_proba() function. It is designed to deal with multi-class predictions. It will return probability estimates for the test data X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.predict_proba(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'random-forest'></a>\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rf_author_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print 'the fitting takes: ', round(time() - t0), 's'\n",
    "\n",
    "t1 = time()\n",
    "pre = clf.predict(features_test)\n",
    "print 'the predict: ', round(time() - t1), 's'\n",
    "\n",
    "acc = accuracy_score(pre, labels_test)\n",
    "print acc\n",
    "\n",
    "\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'adaboost'></a>\n",
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load adaboost_author_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print 'the fitting takes: ', round(time() - t0), 's'\n",
    "\n",
    "t1 = time()\n",
    "pre = clf.predict(features_test)\n",
    "print 'the predict: ', round(time() - t1), 's'\n",
    "\n",
    "acc = accuracy_score(pre, labels_test)\n",
    "print acc\n",
    "\n",
    "\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Enron Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Enron Email Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load explore_enron_data.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    Starter code for exploring the Enron dataset (emails + finances);\n",
    "    loads up the dataset (pickled dict of dicts).\n",
    "\n",
    "    The dataset has the form:\n",
    "    enron_data[\"LASTNAME FIRSTNAME MIDDLEINITIAL\"] = { features_dict }\n",
    "\n",
    "    {features_dict} is a dictionary of features associated with that person.\n",
    "    You should explore features_dict as part of the mini-project,\n",
    "    but here's an example to get you started:\n",
    "\n",
    "    enron_data[\"SKILLING JEFFREY K\"][\"bonus\"] = 5600000\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "enron_data = pickle.load(open(\"../final_project/final_project_dataset.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many people are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(enron_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many features for each person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in enron_data:\n",
    "    print len(enron_data[key])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many people in the dataset actually has person of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key in enron_data:\n",
    "    if enron_data[key]['poi']:\n",
    "        count += 1\n",
    "print count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a text file containing all the POI names. Check how many people are in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('poi_names.txt', 'r') as f:\n",
    "    f.readline()\n",
    "    names = f.read()\n",
    "    names = names.split('\\n')\n",
    "    names = names[1:len(names)-1]\n",
    "    \n",
    "len(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name within the dataset following certain format: 'LAST NAME FIRST NAME' or \n",
    "'LAST NAME FIRST NAME MIDDLE INITIAL'. I am going to check the information for some people. I will write a function to transfer the name to the format I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_name(name):\n",
    "    new_name = name.upper()\n",
    "    new_name = new_name.split(' ')\n",
    "    \n",
    "    if len(new_name) == 2:\n",
    "        new_name = list(reversed(new_name))\n",
    "        new_name = ' '.join(new_name)\n",
    "    elif len(new_name) == 3:\n",
    "        new_order = [2, 0, 1]\n",
    "        new_name = [new_name[i] for i in new_order]\n",
    "        new_name = ' '.join(new_name)\n",
    "    return new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = update_name('Wesley Colwell')\n",
    "enron_data[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = update_name('Jeffrey K Skilling')\n",
    "enron_data[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = update_name('Kenneth L Lay')\n",
    "enron_data[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key in enron_data:\n",
    "    if enron_data[key]['salary'] != 'NaN':\n",
    "        count += 1\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key in enron_data:\n",
    "    if enron_data[key]['email_address'] != 'NaN':\n",
    "        count += 1\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key in enron_data:\n",
    "    if enron_data[key]['total_payments'] != 'NaN':\n",
    "        count += 1\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key in enron_data:\n",
    "    if (enron_data[key]['poi']) and (enron_data[key]['total_payments'] == 'NaN'):\n",
    "        count += 1\n",
    "print count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load finance_regression.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    Starter code for the regression mini-project.\n",
    "\n",
    "    Loads up/formats a modified version of the dataset\n",
    "    (why modified?  we've removed some trouble points\n",
    "    that you'll find yourself in the outliers mini-project).\n",
    "\n",
    "    Draws a little scatterplot of the training/testing data\n",
    "\n",
    "    You fill in the regression code where indicated:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "dictionary = pickle.load( open(\"../final_project/final_project_dataset_modified.pkl\", \"r\") )\n",
    "\n",
    "### list the features you want to look at--first item in the\n",
    "### list will be the \"target\" feature\n",
    "features_list = [\"bonus\", \"salary\"]\n",
    "data = featureFormat( dictionary, features_list, remove_any_zeroes=True)\n",
    "target, features = targetFeatureSplit( data )\n",
    "\n",
    "### training-testing split needed in regression, just like classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.5, random_state=42)\n",
    "train_color = \"b\"\n",
    "test_color = \"r\"\n",
    "\n",
    "\n",
    "\n",
    "### Your regression goes here!\n",
    "### Please name it reg, so that the plotting code below picks it up and\n",
    "### plots it correctly. Don't forget to change the test_color above from \"b\" to\n",
    "### \"r\" to differentiate training points from test points.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(feature_train, target_train)\n",
    "print reg.coef_\n",
    "print reg.intercept_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### draw the scatterplot, with color-coded training and testing points\n",
    "import matplotlib.pyplot as plt\n",
    "for feature, target in zip(feature_test, target_test):\n",
    "    plt.scatter( feature, target, color=test_color )\n",
    "for feature, target in zip(feature_train, target_train):\n",
    "    plt.scatter( feature, target, color=train_color )\n",
    "\n",
    "### labels for the legend\n",
    "plt.scatter(feature_test[0], target_test[0], color=test_color, label=\"test\")\n",
    "plt.scatter(feature_test[0], target_test[0], color=train_color, label=\"train\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### draw the regression line, once it's coded\n",
    "try:\n",
    "    plt.plot( feature_test, reg.predict(feature_test) )\n",
    "except NameError:\n",
    "    pass\n",
    "plt.xlabel(features_list[1])\n",
    "plt.ylabel(features_list[0])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(feature_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(feature_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the relationship to bonus and long-term-incentive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load finance_regression.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    Starter code for the regression mini-project.\n",
    "\n",
    "    Loads up/formats a modified version of the dataset\n",
    "    (why modified?  we've removed some trouble points\n",
    "    that you'll find yourself in the outliers mini-project).\n",
    "\n",
    "    Draws a little scatterplot of the training/testing data\n",
    "\n",
    "    You fill in the regression code where indicated:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "dictionary = pickle.load( open(\"../final_project/final_project_dataset_modified.pkl\", \"r\") )\n",
    "\n",
    "### list the features you want to look at--first item in the\n",
    "### list will be the \"target\" feature\n",
    "features_list = [\"bonus\", \"long_term_incentive\"]\n",
    "data = featureFormat( dictionary, features_list, remove_any_zeroes=True)\n",
    "target, features = targetFeatureSplit( data )\n",
    "\n",
    "### training-testing split needed in regression, just like classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.5, random_state=42)\n",
    "train_color = \"b\"\n",
    "test_color = \"r\"\n",
    "\n",
    "\n",
    "\n",
    "### Your regression goes here!\n",
    "### Please name it reg, so that the plotting code below picks it up and\n",
    "### plots it correctly. Don't forget to change the test_color above from \"b\" to\n",
    "### \"r\" to differentiate training points from test points.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(feature_train, target_train)\n",
    "print reg.coef_\n",
    "print reg.intercept_\n",
    "print reg.score(feature_train, target_train)\n",
    "print reg.score(feature_test, target_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### draw the scatterplot, with color-coded training and testing points\n",
    "import matplotlib.pyplot as plt\n",
    "for feature, target in zip(feature_test, target_test):\n",
    "    plt.scatter( feature, target, color=test_color )\n",
    "for feature, target in zip(feature_train, target_train):\n",
    "    plt.scatter( feature, target, color=train_color )\n",
    "\n",
    "### labels for the legend\n",
    "plt.scatter(feature_test[0], target_test[0], color=test_color, label=\"test\")\n",
    "plt.scatter(feature_test[0], target_test[0], color=train_color, label=\"train\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### draw the regression line, once it's coded\n",
    "try:\n",
    "    plt.plot( feature_test, reg.predict(feature_test) )\n",
    "except NameError:\n",
    "    pass\n",
    "plt.xlabel(features_list[1])\n",
    "plt.ylabel(features_list[0])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load finance_regression.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    Starter code for the regression mini-project.\n",
    "\n",
    "    Loads up/formats a modified version of the dataset\n",
    "    (why modified?  we've removed some trouble points\n",
    "    that you'll find yourself in the outliers mini-project).\n",
    "\n",
    "    Draws a little scatterplot of the training/testing data\n",
    "\n",
    "    You fill in the regression code where indicated:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "dictionary = pickle.load( open(\"../final_project/final_project_dataset_modified.pkl\", \"r\") )\n",
    "\n",
    "### list the features you want to look at--first item in the\n",
    "### list will be the \"target\" feature\n",
    "features_list = [\"bonus\", \"salary\"]\n",
    "data = featureFormat( dictionary, features_list, remove_any_zeroes=True)\n",
    "target, features = targetFeatureSplit( data )\n",
    "\n",
    "### training-testing split needed in regression, just like classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.5, random_state=42)\n",
    "train_color = \"b\"\n",
    "test_color = \"r\"\n",
    "\n",
    "\n",
    "\n",
    "### Your regression goes here!\n",
    "### Please name it reg, so that the plotting code below picks it up and\n",
    "### plots it correctly. Don't forget to change the test_color above from \"b\" to\n",
    "### \"r\" to differentiate training points from test points.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(feature_train, target_train)\n",
    "print \"With Outliers:\"\n",
    "print reg.coef_\n",
    "print reg.intercept_\n",
    "print reg.score(feature_train, target_train)\n",
    "print reg.score(feature_test, target_test)\n",
    "\n",
    "\n",
    "### draw the scatterplot, with color-coded training and testing points\n",
    "import matplotlib.pyplot as plt\n",
    "for feature, target in zip(feature_test, target_test):\n",
    "    plt.scatter( feature, target, color=test_color )\n",
    "for feature, target in zip(feature_train, target_train):\n",
    "    plt.scatter( feature, target, color=train_color )\n",
    "\n",
    "### labels for the legend\n",
    "plt.scatter(feature_test[0], target_test[0], color=test_color, label=\"test\")\n",
    "plt.scatter(feature_test[0], target_test[0], color=train_color, label=\"train\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### draw the regression line, once it's coded\n",
    "try:\n",
    "    plt.plot( feature_test, reg.predict(feature_test) )\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "reg.fit(feature_test, target_test)\n",
    "print \"Without Outliers\"\n",
    "print reg.coef_\n",
    "print reg.intercept_\n",
    "print reg.score(feature_train, target_train)\n",
    "print reg.score(feature_test, target_test)\n",
    "plt.plot(feature_train, reg.predict(feature_train), color=\"b\")\n",
    "\n",
    "plt.xlabel(features_list[1])\n",
    "plt.ylabel(features_list[0])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load outlier_removal_regression.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import random\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from outlier_cleaner import outlierCleaner\n",
    "\n",
    "\n",
    "### load up some practice data with outliers in it\n",
    "ages = pickle.load( open(\"practice_outliers_ages.pkl\", \"r\") )\n",
    "net_worths = pickle.load( open(\"practice_outliers_net_worths.pkl\", \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### ages and net_worths need to be reshaped into 2D numpy arrays\n",
    "### second argument of reshape command is a tuple of integers: (n_rows, n_columns)\n",
    "### by convention, n_rows is the number of data points\n",
    "### and n_columns is the number of features\n",
    "ages       = numpy.reshape( numpy.array(ages), (len(ages), 1))\n",
    "net_worths = numpy.reshape( numpy.array(net_worths), (len(net_worths), 1))\n",
    "from sklearn.cross_validation import train_test_split\n",
    "ages_train, ages_test, net_worths_train, net_worths_test = train_test_split(ages, net_worths, test_size=0.1, random_state=42)\n",
    "\n",
    "### fill in a regression here!  Name the regression object reg so that\n",
    "### the plotting code below works, and you can see what your regression looks like\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(ages_train, net_worths_train)\n",
    "print \"slope: \", reg.coef_\n",
    "print \"intercept:\", reg.intercept_\n",
    "\n",
    "try:\n",
    "    plt.plot(ages, reg.predict(ages), color=\"blue\")\n",
    "except NameError:\n",
    "    pass\n",
    "plt.scatter(ages, net_worths)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### identify and remove the most outlier-y points\n",
    "cleaned_data = []\n",
    "try:\n",
    "    predictions = reg.predict(ages_train)\n",
    "    cleaned_data = outlierCleaner( predictions, ages_train, net_worths_train )\n",
    "except NameError:\n",
    "    print \"your regression object doesn't exist, or isn't name reg\"\n",
    "    print \"can't make predictions to use in identifying outliers\"\n",
    "\n",
    "### only run this code if cleaned_data is returning data\n",
    "if len(cleaned_data) > 0:\n",
    "    ages, net_worths, errors = zip(*cleaned_data)\n",
    "    ages       = numpy.reshape( numpy.array(ages), (len(ages), 1))\n",
    "    net_worths = numpy.reshape( numpy.array(net_worths), (len(net_worths), 1))\n",
    "\n",
    "    ### refit your cleaned data!\n",
    "    try:\n",
    "        reg.fit(ages, net_worths)\n",
    "        plt.plot(ages, reg.predict(ages), color=\"blue\")\n",
    "    except NameError:\n",
    "        print \"you don't seem to have regression imported/created,\"\n",
    "        print \"   or else your regression object isn't named reg\"\n",
    "        print \"   either way, only draw the scatter plot of the cleaned data\"\n",
    "    plt.scatter(ages, net_worths)\n",
    "    plt.xlabel(\"ages\")\n",
    "    plt.ylabel(\"net worths\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "else:\n",
    "    print \"outlierCleaner() is returning an empty list, no refitting to be done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(ages_test, net_worths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load outlier_removal_regression.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import random\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from outlier_cleaner import outlierCleaner\n",
    "\n",
    "\n",
    "### load up some practice data with outliers in it\n",
    "ages = pickle.load( open(\"practice_outliers_ages.pkl\", \"r\") )\n",
    "net_worths = pickle.load( open(\"practice_outliers_net_worths.pkl\", \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### ages and net_worths need to be reshaped into 2D numpy arrays\n",
    "### second argument of reshape command is a tuple of integers: (n_rows, n_columns)\n",
    "### by convention, n_rows is the number of data points\n",
    "### and n_columns is the number of features\n",
    "ages       = numpy.reshape( numpy.array(ages), (len(ages), 1))\n",
    "net_worths = numpy.reshape( numpy.array(net_worths), (len(net_worths), 1))\n",
    "from sklearn.cross_validation import train_test_split\n",
    "ages_train, ages_test, net_worths_train, net_worths_test = train_test_split(ages, net_worths, test_size=0.1, random_state=42)\n",
    "\n",
    "### fill in a regression here!  Name the regression object reg so that\n",
    "### the plotting code below works, and you can see what your regression looks like\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(ages_train, net_worths_train)\n",
    "print \"slope: \", reg.coef_\n",
    "print \"intercept:\", reg.intercept_\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    plt.plot(ages, reg.predict(ages), color=\"blue\")\n",
    "except NameError:\n",
    "    pass\n",
    "plt.scatter(ages, net_worths)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### identify and remove the most outlier-y points\n",
    "cleaned_data = []\n",
    "try:\n",
    "    predictions = reg.predict(ages_train)\n",
    "    cleaned_data = outlierCleaner( predictions, ages_train, net_worths_train )\n",
    "except NameError:\n",
    "    print \"your regression object doesn't exist, or isn't name reg\"\n",
    "    print \"can't make predictions to use in identifying outliers\"\n",
    "\n",
    "\n",
    "### only run this code if cleaned_data is returning data\n",
    "if len(cleaned_data) > 0:\n",
    "    ages, net_worths, errors = zip(*cleaned_data)\n",
    "    ages       = numpy.reshape( numpy.array(ages), (len(ages), 1))\n",
    "    net_worths = numpy.reshape( numpy.array(net_worths), (len(net_worths), 1))\n",
    "\n",
    "    ### refit your cleaned data!\n",
    "    try:\n",
    "        reg.fit(ages, net_worths)\n",
    "        plt.plot(ages, reg.predict(ages), color=\"blue\")\n",
    "    except NameError:\n",
    "        print \"you don't seem to have regression imported/created,\"\n",
    "        print \"   or else your regression object isn't named reg\"\n",
    "        print \"   either way, only draw the scatter plot of the cleaned data\"\n",
    "    plt.scatter(ages, net_worths)\n",
    "    plt.xlabel(\"ages\")\n",
    "    plt.ylabel(\"net worths\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "else:\n",
    "    print \"outlierCleaner() is returning an empty list, no refitting to be done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(ages_test, net_worths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load enron_outliers.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "### read in data dictionary, convert to numpy array\n",
    "data_dict = pickle.load( open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "features = [\"salary\", \"bonus\"]\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "\n",
    "### your code below\n",
    "for item in data:\n",
    "    salary = item[0]\n",
    "    bonus = item[1]\n",
    "    plt.scatter(salary, bonus)\n",
    "\n",
    "plt.xlabel('salary')\n",
    "plt.ylabel('bonus')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a big outlier. I found it is \"total\" in the dataset that we need to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load enron_outliers.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "### read in data dictionary, convert to numpy array\n",
    "data_dict = pickle.load( open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "features = [\"salary\", \"bonus\"]\n",
    "data_dict.pop(\"TOTAL\", 0)\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "\n",
    "### your code below\n",
    "for item in data:\n",
    "    salary = item[0]\n",
    "    bonus = item[1]\n",
    "    plt.scatter(salary, bonus)\n",
    "\n",
    "plt.xlabel('salary')\n",
    "plt.ylabel('bonus')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load k_means_cluster.py\n",
    "#!/usr/bin/python \n",
    "\n",
    "\"\"\" \n",
    "    Skeleton code for k-means clustering mini-project.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Draw(pred, features, poi, mark_poi=False, name=\"image.png\", f1_name=\"feature 1\", f2_name=\"feature 2\"):\n",
    "    \"\"\" some plotting code designed to help you visualize your clusters \"\"\"\n",
    "\n",
    "    ### plot each cluster with a different color--add more colors for\n",
    "    ### drawing more than five clusters\n",
    "    colors = [\"b\", \"c\", \"k\", \"m\", \"g\"]\n",
    "    for ii, pp in enumerate(pred):\n",
    "        plt.scatter(features[ii][0], features[ii][1], color = colors[pred[ii]])\n",
    "\n",
    "    ### if you like, place red stars over points that are POIs (just for funsies)\n",
    "    if mark_poi:\n",
    "        for ii, pp in enumerate(pred):\n",
    "            if poi[ii]:\n",
    "                plt.scatter(features[ii][0], features[ii][1], color=\"r\", marker=\"*\")\n",
    "    plt.xlabel(f1_name)\n",
    "    plt.ylabel(f2_name)\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "### load in the dict of dicts containing all the data on each person in the dataset\n",
    "data_dict = pickle.load( open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "### there's an outlier--remove it! \n",
    "data_dict.pop(\"TOTAL\", 0)\n",
    "\n",
    "\n",
    "### the input features we want to use \n",
    "### can be any key in the person-level dictionary (salary, director_fees, etc.) \n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\"\n",
    "poi  = \"poi\"\n",
    "features_list = [poi, feature_1, feature_2]\n",
    "data = featureFormat(data_dict, features_list )\n",
    "poi, finance_features = targetFeatureSplit( data )\n",
    "\n",
    "\n",
    "### in the \"clustering with 3 features\" part of the mini-project,\n",
    "### you'll want to change this line to \n",
    "### for f1, f2, _ in finance_features:\n",
    "### (as it's currently written, the line below assumes 2 features)\n",
    "for f1, f2 in finance_features:\n",
    "    plt.scatter( f1, f2 )\n",
    "plt.show()\n",
    "\n",
    "### cluster here; create predictions of the cluster labels\n",
    "### for the data and store them to a list called pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### rename the \"name\" parameter when you change the number of features\n",
    "### so that the figure gets saved to a different file\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False, name=\"clusters.pdf\", f1_name=feature_1, f2_name=feature_2)\n",
    "except NameError:\n",
    "    print \"no predictions object named pred found, no clusters to plot\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load k_means_cluster.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    Skeleton code for k-means clustering mini-project.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Draw(pred, features, poi, mark_poi=False, name=\"image.png\", f1_name=\"feature 1\", f2_name=\"feature 2\"):\n",
    "    \"\"\" some plotting code designed to help you visualize your clusters \"\"\"\n",
    "\n",
    "    ### plot each cluster with a different color--add more colors for\n",
    "    ### drawing more than five clusters\n",
    "    colors = [\"b\", \"c\", \"k\", \"m\", \"g\"]\n",
    "    for ii, pp in enumerate(pred):\n",
    "        plt.scatter(features[ii][0], features[ii][1], color = colors[pred[ii]])\n",
    "\n",
    "    ### if you like, place red stars over points that are POIs (just for funsies)\n",
    "    if mark_poi:\n",
    "        for ii, pp in enumerate(pred):\n",
    "            if poi[ii]:\n",
    "                plt.scatter(features[ii][0], features[ii][1], color=\"r\", marker=\"*\")\n",
    "    plt.xlabel(f1_name)\n",
    "    plt.ylabel(f2_name)\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "### load in the dict of dicts containing all the data on each person in the dataset\n",
    "data_dict = pickle.load( open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "### there's an outlier--remove it!\n",
    "data_dict.pop(\"TOTAL\", 0)\n",
    "\n",
    "\n",
    "### the input features we want to use\n",
    "### can be any key in the person-level dictionary (salary, director_fees, etc.)\n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\"\n",
    "poi  = \"poi\"\n",
    "features_list = [poi, feature_1, feature_2]\n",
    "data = featureFormat(data_dict, features_list )\n",
    "poi, finance_features = targetFeatureSplit( data )\n",
    "\n",
    "\n",
    "### in the \"clustering with 3 features\" part of the mini-project,\n",
    "### you'll want to change this line to\n",
    "### for f1, f2, _ in finance_features:\n",
    "### (as it's currently written, the line below assumes 2 features)\n",
    "for f1, f2 in finance_features:\n",
    "    plt.scatter( f1, f2 )\n",
    "plt.show()\n",
    "\n",
    "### cluster here; create predictions of the cluster labels\n",
    "### for the data and store them to a list called pred\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "pred = kmeans.fit_predict(finance_features)\n",
    "\n",
    "\n",
    "\n",
    "### rename the \"name\" parameter when you change the number of features\n",
    "### so that the figure gets saved to a different file\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False, name=\"clusters.pdf\", f1_name=feature_1, f2_name=feature_2)\n",
    "except NameError:\n",
    "    print \"no predictions object named pred found, no clusters to plot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load k_means_cluster.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    Skeleton code for k-means clustering mini-project.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Draw(pred, features, poi, mark_poi=False, name=\"image.png\", f1_name=\"feature 1\", f2_name=\"feature 2\"):\n",
    "    \"\"\" some plotting code designed to help you visualize your clusters \"\"\"\n",
    "\n",
    "    ### plot each cluster with a different color--add more colors for\n",
    "    ### drawing more than five clusters\n",
    "    colors = [\"b\", \"c\", \"k\", \"m\", \"g\"]\n",
    "    for ii, pp in enumerate(pred):\n",
    "        plt.scatter(features[ii][0], features[ii][1], color = colors[pred[ii]])\n",
    "\n",
    "    ### if you like, place red stars over points that are POIs (just for funsies)\n",
    "    if mark_poi:\n",
    "        for ii, pp in enumerate(pred):\n",
    "            if poi[ii]:\n",
    "                plt.scatter(features[ii][0], features[ii][1], color=\"r\", marker=\"*\")\n",
    "    plt.xlabel(f1_name)\n",
    "    plt.ylabel(f2_name)\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "### load in the dict of dicts containing all the data on each person in the dataset\n",
    "data_dict = pickle.load( open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "### there's an outlier--remove it!\n",
    "data_dict.pop(\"TOTAL\", 0)\n",
    "\n",
    "\n",
    "### the input features we want to use\n",
    "### can be any key in the person-level dictionary (salary, director_fees, etc.)\n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\"\n",
    "feature_3 = 'total_payments'\n",
    "poi  = \"poi\"\n",
    "features_list = [poi, feature_1, feature_2, feature_3]\n",
    "data = featureFormat(data_dict, features_list )\n",
    "poi, finance_features = targetFeatureSplit( data )\n",
    "\n",
    "\n",
    "### in the \"clustering with 3 features\" part of the mini-project,\n",
    "### you'll want to change this line to\n",
    "### for f1, f2, _ in finance_features:\n",
    "### (as it's currently written, the line below assumes 2 features)\n",
    "for f1, f2, _ in finance_features:\n",
    "    plt.scatter( f1, f2 )\n",
    "plt.show()\n",
    "\n",
    "### cluster here; create predictions of the cluster labels\n",
    "### for the data and store them to a list called pred\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "pred = kmeans.fit_predict(finance_features)\n",
    "\n",
    "\n",
    "\n",
    "### rename the \"name\" parameter when you change the number of features\n",
    "### so that the figure gets saved to a different file\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False, name=\"clusters.pdf\", f1_name=feature_1, f2_name=feature_2)\n",
    "except NameError:\n",
    "    print \"no predictions object named pred found, no clusters to plot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = [data_dict[key]['exercised_stock_options'] for key in data_dict\n",
    "         if data_dict[key]['exercised_stock_options'] != 'NaN']\n",
    "print \"min: \", min(stock)\n",
    "print \"max: \", max(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_salary = float(\"+inf\")\n",
    "max_salary = float(\"-inf\")\n",
    "\n",
    "for key in data_dict:\n",
    "    salary = data_dict[key]['salary']\n",
    "    if salary != \"NaN\":\n",
    "        if salary > max_salary:\n",
    "            max_salary = salary\n",
    "        if salary < min_salary:\n",
    "            min_salary = salary\n",
    "\n",
    "print \"min: \", min_salary\n",
    "print \"max: \", max_salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling with 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled = min_max_scaler.fit(finance_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled.transform([[200000, 1000000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hi Everyone  If you can read this message youre properly using parseOutText  Please proceed to the next part of the project\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load parse_out_email_text.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### project part 2: comment out the line below\n",
    "        words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return words\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "    text = parseOutText(ff)\n",
    "    print text\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project\n"
     ]
    }
   ],
   "source": [
    "# %load parse_out_email_text.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated)\n",
    "\n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    new_words = []\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### project part 2: comment out the line below\n",
    "        # words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        stemmer = SnowballStemmer('english')\n",
    "\n",
    "        old_words = text_string.split()\n",
    "\n",
    "        for word in old_words:\n",
    "            word = stemmer.stem(word)\n",
    "            new_words.append(word)\n",
    "\n",
    "        words = ' '.join(new_words)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "    text = parseOutText(ff)\n",
    "    print text\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n"
     ]
    }
   ],
   "source": [
    "# %load vectorize_text.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append( \"../tools/\" )\n",
    "from parse_out_email_text import parseOutText\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "remove_list = [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        temp_counter += 1\n",
    "        if temp_counter < 200:\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            email = open(path, \"r\")\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            words = parseOutText(email)\n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            for item in remove_list:\n",
    "                words = words.replace(item, '')\n",
    "\n",
    "            ### append the text to word_data\n",
    "            word_data.append(words)\n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            from_data.append(0 if name == 'Sara' else 1)\n",
    "\n",
    "            email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### in Part 4, do TfIdf vectorization here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'tjonesnsf stephani and sam need nymex calendar'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data[152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n"
     ]
    }
   ],
   "source": [
    "# %load vectorize_text.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append( \"../tools/\" )\n",
    "from parse_out_email_text import parseOutText\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "\n",
    "remove_list = [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "\n",
    "\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            email = open(path, \"r\")\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            words = parseOutText(email)\n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            for item in remove_list:\n",
    "                words = words.replace(item, '')\n",
    "\n",
    "            ### append the text to word_data\n",
    "            word_data.append(words)\n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            from_data.append(0 if name == 'sara' else 1)\n",
    "\n",
    "            email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### in Part 4, do TfIdf vectorization here\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "new_word_data = tfidf.fit_transform(word_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38757"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tfidf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'stephaniethank'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()[34597]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/python2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947667804323\n"
     ]
    }
   ],
   "source": [
    "# %load find_signature.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../text_learning/your_word_data.pkl\"\n",
    "authors_file = \"../text_learning/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150]\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "predict = clf.predict(features_test)\n",
    "\n",
    "print accuracy_score(predict, labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_words = tfidf.get_feature_names()\n",
    "feature_importances = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature number 1: shirt (0.764705882353)\n",
      "feature number 2: secreto (0.134028294862)\n",
      "feature number 3: forwardene113000doc (0.0749500333111)\n",
      "feature number 4: king (0.0263157894737)\n",
      "feature number 5: basic (0.0)\n",
      "feature number 6: bass (0.0)\n",
      "feature number 7: basp (0.0)\n",
      "feature number 8: basketbal (0.0)\n",
      "feature number 9: basket (0.0)\n",
      "feature number 10: basisthank (0.0)\n"
     ]
    }
   ],
   "source": [
    "new_order = np.argsort(feature_importances)\n",
    "\n",
    "for i in range(10):\n",
    "    print \"feature number {}: {} ({})\".format(i+1, feature_words[new_order[i]], feature_importances[new_order[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33614"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_words.index('shirt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'shirt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_words[33614]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947667804323\n",
      "feature number 1: sshacklensf (0.764705882353)\n",
      "feature number 2: smith (0.134028294862)\n",
      "feature number 3: fyi (0.0749500333111)\n",
      "feature number 4: leav (0.0263157894737)\n",
      "feature number 5: bellhouectect (0.0)\n",
      "feature number 6: bellvill (0.0)\n",
      "feature number 7: bellsouth (0.0)\n",
      "feature number 8: belllonectect (0.0)\n",
      "feature number 9: bellissimo (0.0)\n",
      "feature number 10: bellisario (0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33614"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../text_learning/your_word_data.pkl\"\n",
    "authors_file = \"../text_learning/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150]\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "predict = clf.predict(features_test)\n",
    "\n",
    "print accuracy_score(predict, labels_test)\n",
    "\n",
    "feature_words = vectorizer.get_feature_names()\n",
    "feature_importances = clf.feature_importances_\n",
    "new_order = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "for i in range(10):\n",
    "    print \"feature number {}: {} ({})\".format(i+1, feature_words[new_order[i]], feature_importances[new_order[i]])\n",
    "\n",
    "feature_words.index(feature_words[new_order[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the significant word, and refit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n"
     ]
    }
   ],
   "source": [
    "# %load vectorize_text.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append( \"../tools/\" )\n",
    "from parse_out_email_text import parseOutText\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "\n",
    "remove_list = [\"sara\", \"shackleton\", \"chris\", \"germani\", \"sshacklensf\"]\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "\n",
    "\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            email = open(path, \"r\")\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            words = parseOutText(email)\n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            for item in remove_list:\n",
    "                words = words.replace(item, '')\n",
    "\n",
    "            ### append the text to word_data\n",
    "            word_data.append(words)\n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            from_data.append(0 if name == 'sara' else 1)\n",
    "\n",
    "            email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### in Part 4, do TfIdf vectorization here\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "new_word_data = tfidf.fit_transform(word_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.950511945392\n"
     ]
    }
   ],
   "source": [
    "%run find_signature.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature number 1: cgermannsf (0.666666666667)\n",
      "feature number 2: 62502pst (0.162601626016)\n",
      "feature number 3: deal (0.093808630394)\n",
      "feature number 4: cgerman (0.0506072874494)\n",
      "feature number 5: trade (0.0263157894737)\n",
      "feature number 6: bellhouectect (0.0)\n",
      "feature number 7: bellvill (0.0)\n",
      "feature number 8: bellsouth (0.0)\n",
      "feature number 9: belllonectect (0.0)\n",
      "feature number 10: bellissimo (0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14343"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_words = vectorizer.get_feature_names()\n",
    "feature_importances = clf.feature_importances_\n",
    "new_order = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "for i in range(10):\n",
    "    print \"feature number {}: {} ({})\".format(i+1, feature_words[new_order[i]], feature_importances[new_order[i]])\n",
    "\n",
    "feature_words.index(feature_words[new_order[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
